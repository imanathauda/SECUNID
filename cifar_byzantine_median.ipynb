{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOko1BfXFfQLXXeLxg6EF7D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":671},"id":"JYI6UoGfFT6K","executionInfo":{"status":"ok","timestamp":1714892791434,"user_tz":-330,"elapsed":269806,"user":{"displayName":"Iman Athauda","userId":"04736040470579387820"}},"outputId":"e68e1072-665f-4bfc-f7ac-0bfeb1bcffcf"},"outputs":[{"output_type":"stream","name":"stdout","text":["device:  cpu\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Current learning rate: 0.0015\n","Iteration 1 : main_model accuracy on all test data:  0.1043\n"]},{"output_type":"execute_result","data":{"text/plain":["<module 'matplotlib.pyplot' from '/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py'>"]},"metadata":{},"execution_count":4},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x600 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","import numpy as np\n","from torch.optim import lr_scheduler\n","import matplotlib.pyplot as plt  # Added for plotting\n","import pandas as pd  # Added for CSV file handling\n","\n","\n","# Contents of construct_models.py\n","\n","import torch.nn.functional as F\n","\n","def weights_init(model, torch_manual_seed=2304):\n","    torch.manual_seed(torch_manual_seed)\n","    torch.cuda.manual_seed_all(torch_manual_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","        elif isinstance(m, nn.Linear):\n","            nn.init.xavier_normal_(m.weight)\n","            nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.BatchNorm2d):\n","            nn.init.constant_(m.weight, 1)\n","            nn.init.constant_(m.bias, 0)\n","\n","def learning_rate_decay(optimizer_dict, decay_rate):\n","    for i in range(len(optimizer_dict)):\n","        optimizer_name = \"optimizer\" + str(i)\n","        old_lr = optimizer_dict[optimizer_name].param_groups[0][\"lr\"]\n","        optimizer_dict[optimizer_name].param_groups[0][\"lr\"] = old_lr * decay_rate\n","    return optimizer_dict\n","\n","def update_learning_rate_decay(optimizer_dict, new_lr):\n","    for i in range(len(optimizer_dict)):\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_dict[optimizer_name].param_groups[0][\"lr\"] = new_lr\n","    return optimizer_dict\n","\n","# Model architectures\n","class Net2nn(nn.Module):\n","    def __init__(self):\n","        super(Net2nn, self).__init__()\n","        self.fc1 = nn.Linear(784, 200)\n","        self.fc2 = nn.Linear(200, 200)\n","        self.fc3 = nn.Linear(200, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","class Netcnn(nn.Module):\n","    def __init__(self):\n","        super(Netcnn, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5)\n","        self.conv2 = nn.Conv2d(32, 64, 5)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","\n","class Netcnn_cifar(nn.Module):\n","    def __init__(self):\n","        super(Netcnn_cifar, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3)\n","        self.conv2 = nn.Conv2d(32, 64, 3)\n","        self.conv3 = nn.Conv2d(64, 64, 3)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv3(x))\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","class Cifar10CNN(nn.Module):\n","\n","    def __init__(self):\n","        super(Cifar10CNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","\n","        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","\n","\n","        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n","        # self.dropout1 = nn.Dropout()\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","\n","\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = F.max_pool2d(x, 2)\n","\n","        x = F.relu(self.conv5(x))\n","        x = F.relu(self.conv6(x))\n","        x = F.max_pool2d(x, 2)\n","\n","        x = x.view(-1, 128 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        # x = self.dropout1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","class Net_fashion(nn.Module):\n","    def __init__(self):\n","        super(Net_fashion, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = x.view(-1, 64 * 7 * 7)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","# Contents of train_nodes.py\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","from torchvision import models\n","from torchvision import transforms\n","from statistics import NormalDist\n","from scipy import stats\n","\n","\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","def train_with_clipping(model, train_loader, criterion, optimizer, device, clipping=True, clipping_threshold=10):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        if clipping:\n","            torch.nn.utils.clip_grad_value_(model.parameters(), clipping_threshold)\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","\n","def train_with_augmentation(model, train_loader, criterion, optimizer, device, clipping, clipping_threshold=10, use_augmentation=False, augment=None ):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","\n","        if use_augmentation:\n","            data = augment(data)\n","\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        if clipping:\n","            torch.nn.utils.clip_grad_value_(model.parameters(), clipping_threshold)\n","\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","\n","def validation(model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0.0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","\n","            test_loss += criterion(output, target).item()\n","            prediction = output.argmax(dim=1, keepdim=True)\n","            correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    test_loss /= len(test_loader)\n","    correct /= len(test_loader.dataset)\n","\n","    return (test_loss, correct)\n","\n","\n","def get_model_names(model_dict):\n","    name_of_models = list(model_dict.keys())\n","    return name_of_models\n","\n","\n","def get_optimizer_names(optimizer_dict):\n","    name_of_optimizers = list(optimizer_dict.keys())\n","    return name_of_optimizers\n","\n","\n","def get_criterion_names(criterion_dict):\n","    name_of_criterions = list(criterion_dict.keys())\n","    return name_of_criterions\n","\n","\n","def get_x_train_sets_names(x_train_dict):\n","    name_of_x_train_sets = list(x_train_dict.keys())\n","    return name_of_x_train_sets\n","\n","\n","def get_y_train_sets_names(y_train_dict):\n","    name_of_y_train_sets = list(y_train_dict.keys())\n","    return name_of_y_train_sets\n","\n","\n","def get_x_valid_sets_names(x_valid_dict):\n","    name_of_x_valid_sets = list(x_valid_dict.keys())\n","    return name_of_x_valid_sets\n","\n","\n","def get_y_valid_sets_names(y_valid_dict):\n","    name_of_y_valid_sets = list(y_valid_dict.keys())\n","    return name_of_y_valid_sets\n","\n","\n","def get_x_test_sets_names(x_test_dict):\n","    name_of_x_test_sets = list(x_test_dict.keys())\n","    return name_of_x_test_sets\n","\n","\n","def get_y_test_sets_names(y_test_dict):\n","    name_of_y_test_sets = list(y_test_dict.keys())\n","    return name_of_y_test_sets\n","\n","\n","def create_model_optimizer_criterion_dict_for_mnist(number_of_samples, learning_rate, momentum, device, is_cnn=False, weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","        if is_cnn:\n","            model_info = Netcnn()\n","        else:\n","            model_info = Net2nn()\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def create_model_optimizer_criterion_dict_for_cifar_net(number_of_samples, learning_rate, momentum, device,\n","                                                        weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Netcnn_cifar()\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","def create_model_optimizer_criterion_dict_for_cifar_cnn(number_of_samples, learning_rate, momentum, device,\n","                                                        weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Cifar10CNN()\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum,\n","                                         weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","def create_model_optimizer_criterion_dict_for_fashion_mnist(number_of_samples, learning_rate, momentum, device, weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Net_fashion()\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def create_model_optimizer_criterion_dict_for_cifar_resnet(number_of_samples, learning_rate, momentum, device,\n","                                                           weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = models.resnet18(num_classes=10)\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum,\n","                                         weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples):\n","    name_of_models = list(model_dict.keys())\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for i in range(number_of_samples):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(main_model_param_data_list)):\n","                sample_param_data_list[j].data = main_model_param_data_list[j].data.clone()\n","    return model_dict\n","\n","\n","def compare_local_and_merged_model_performance(number_of_samples, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, main_model,\n","                                               main_criterion, device):\n","    accuracy_table = pd.DataFrame(data=np.zeros((number_of_samples, 3)),\n","                                  columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n","\n","    name_of_x_test_sets = list(x_test_dict.keys())\n","    name_of_y_test_sets = list(y_test_dict.keys())\n","\n","    name_of_models = list(model_dict.keys())\n","    name_of_criterions = list(criterion_dict.keys())\n","\n","    for i in range(number_of_samples):\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","\n","        individual_loss, individual_accuracy = validation(model, test_dl, criterion, device)\n","        main_loss, main_accuracy = validation(main_model, test_dl, main_criterion, device)\n","\n","        accuracy_table.loc[i, \"sample\"] = \"sample \" + str(i)\n","        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n","        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n","\n","    return accuracy_table\n","\n","\n","def start_train_end_node_process_without_print(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                               device):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","def start_train_end_node_process_with_cliiping(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                               device, clipping=True, clipping_threshold=10):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_clipping(model, train_dl, criterion, optimizer, device, clipping, clipping_threshold)\n","\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","\n","def start_train_end_node_process_cifar(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                       batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                       device,clipping=False, clipping_threshold =10):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    transform_augment = transforms.Compose(\n","        [\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop((32, 32), padding=4)])\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_augmentation(model, train_dl, criterion, optimizer, device,\n","                                                                 clipping=clipping,\n","                                                                 clipping_threshold=clipping_threshold,\n","                                                                 use_augmentation=True, augment=transform_augment)\n","\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","##########################################\n","\n","def start_train_end_node_process_byzantine_for_cifar_with_augmentation(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch, byzantine_node_list,\n","                                            byzantine_mean, byzantine_std, device, clipping=False, clipping_threshold =10, iteration_byzantine_seed=None ):\n","\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    transform_augment = transforms.Compose(\n","        [\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop((32, 32), padding=4)])\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_augmentation(model, train_dl, criterion, optimizer, device, clipping=clipping, clipping_threshold=clipping_threshold,\n","                                                                 use_augmentation=True, augment=transform_augment)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","    with torch.no_grad():\n","\n","        for j in byzantine_node_list:\n","\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","\n","            for k in range(len(hostile_node_param_data_list)):\n","                np.random.seed(iteration_byzantine_seed)\n","                hostile_node_param_data_list[k].data = torch.tensor(np.random.normal(byzantine_mean,byzantine_std, hostile_node_param_data_list[k].data.shape ), dtype=torch.float32, device=device)\n","\n","            model_dict[name_of_models[j]].float()\n","\n","\n","###############################################\n","\n","def start_train_end_node_process_byzantine(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict,\n","                                               numEpoch, byzantine_node_list, byzantine_mean, byzantine_std, device, iteration_byzantine_seed=None ):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","    with torch.no_grad():\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","\n","            for k in range(len(hostile_node_param_data_list)):\n","                np.random.seed(iteration_byzantine_seed)\n","                hostile_node_param_data_list[k].data = torch.tensor(np.random.normal(byzantine_mean,byzantine_std, hostile_node_param_data_list[k].data.shape ), dtype=torch.float32, device=device)\n","\n","            model_dict[name_of_models[j]].float()\n","\n","\n","################################################\n","\n","def calculate_euclidean_distances(main_model, model_dict):\n","    calculated_parameter_names = []\n","\n","    for parameters in main_model.named_parameters():  ## bias dataları için distance hesaplamıyorum\n","        if \"bias\" not in parameters[0]:\n","            calculated_parameter_names.append(parameters[0])\n","\n","    columns = [\"model\"] + calculated_parameter_names\n","    distances = pd.DataFrame(columns=columns)\n","    model_names = list(model_dict.keys())\n","\n","    main_model_weight_dict = {}\n","    for parameter in main_model.named_parameters():\n","        name = parameter[0]\n","        weight_info = parameter[1]\n","        main_model_weight_dict.update({name: weight_info})\n","\n","    with torch.no_grad():\n","        for i in range(len(model_names)):\n","            distances.loc[i, \"model\"] = model_names[i]\n","            sample_node_parameter_list = list(model_dict[model_names[i]].named_parameters())\n","            for j in sample_node_parameter_list:\n","                if j[0] in calculated_parameter_names:\n","                    distances.loc[i, j[0]] = round(\n","                        np.linalg.norm(main_model_weight_dict[j[0]].cpu().data - j[1].cpu().data), 4)\n","\n","    return distances\n","\n","\n","def calculate_lower_and_upper_limit(data, factor):\n","    quantiles = data.quantile(q=[0.25, 0.50, 0.75]).values\n","    q1 = quantiles[0]\n","    q2 = quantiles[1]\n","    q3 = quantiles[2]\n","    iqr = q3 - q1\n","    lower_limit = q1 - factor * iqr\n","    upper_limit = q3 + factor * iqr\n","    return lower_limit, upper_limit\n","\n","\n","def get_outlier_situation_and_thresholds_for_layers(distances, factor=1.5):\n","    layers = list(distances.columns)\n","    layers.remove(\"model\")\n","    threshold_columns = []\n","    for layer in layers:\n","        threshold_columns.append((layer + \"_lower\"))\n","        threshold_columns.append((layer + \"_upper\"))\n","    thresholds = pd.DataFrame(columns=threshold_columns)\n","\n","    include_calculation_result = True\n","    for layer in layers:\n","        data = distances[layer]\n","        lower, upper = calculate_lower_and_upper_limit(data, factor)\n","        lower_name = layer + \"_lower\"\n","        upper_name = layer + \"_upper\"\n","        thresholds.loc[0, lower_name] = lower\n","        thresholds.loc[0, upper_name] = upper\n","        name = layer + \"_is_in_ci\"\n","\n","        distances[name] = (distances[layer] > lower) & (distances[layer] < upper)\n","        include_calculation_result = include_calculation_result & distances[name]\n","\n","    distances[\"include_calculation\"] = include_calculation_result\n","    return distances, thresholds\n","\n","\n","def compare_individual_models_on_only_one_label(model_dict, criterion_dict, x_just_dict, y_just_dict, batch_size,\n","                                                device):\n","    columns = [\"model_name\"]\n","    label_names = []\n","    for l in range(10):\n","        label_names.append(\"label\" + str(l))\n","        columns.append(\"label\" + str(l))\n","\n","    accuracy_rec = pd.DataFrame(data=np.zeros([10, 11]), columns=columns)\n","\n","    # x_just_dict, y_just_dict = create_just_data(x_test, y_test, x_just_name=\"x_test_just_\", y_just_name=\"y_test_just_\")\n","\n","    name_of_x_test_just_sets = list(x_just_dict.keys())\n","    name_of_y_test_just_sets = list(y_just_dict.keys())\n","    name_of_models = list(model_dict.keys())\n","    name_of_criterions = list(criterion_dict.keys())\n","\n","    for i in range(len(name_of_models)):\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","\n","        accuracy_rec.loc[i, \"model_name\"] = name_of_models[i]\n","\n","        for j in range(10):\n","            x_test_just = x_just_dict[name_of_x_test_just_sets[j]]\n","            y_test_just = y_just_dict[name_of_y_test_just_sets[j]]\n","\n","            test_ds_just = TensorDataset(x_test_just, y_test_just)\n","            test_dl_just = DataLoader(test_ds_just, batch_size=batch_size * 2)\n","\n","            test_loss, test_accuracy = validation(model, test_dl_just, criterion, device)\n","\n","            accuracy_rec.loc[i, label_names[j]] = test_accuracy\n","    #             print( name_of_models[i], \">>\" ,j, \" tahmin etmesi: {:7.4f}\".format(test_accuracy))\n","    #         print(\"******************\")\n","    return accuracy_rec\n","\n","\n","def get_averaged_weights_faster(model_dict, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        mean_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","\n","    return mean_weight_array\n","\n","\n","def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device):\n","    mean_weight_array = get_averaged_weights_faster(model_dict, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = mean_weight_array[j]\n","    return main_model\n","\n","\n","def get_coordinate_wise_median_of_weights(model_dict, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        median_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            median_weight_array.append(torch.median(weight_dict[weight_names_list[m]], 0).values)\n","\n","    return median_weight_array\n","\n","def set_coordinatewise_med_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device):\n","    median_weight_array = get_coordinate_wise_median_of_weights(model_dict, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = median_weight_array[j]\n","    return main_model\n","\n","\n","\n","\n","\n","\n","\n","\n","def get_averaged_weights_without_outliers_strict_condition(model_dict, iteration_distance, device):\n","    chosen_clients = iteration_distance[iteration_distance[\"include_calculation\"] == True].index\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","\n","    ### mesela conv 1 için zeros [chosen client kadar, 32, 1, 5, 5] atanıyor bunları doldurup mean alacağız\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(chosen_clients))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(chosen_clients)):\n","            sample_param_data_list = list(model_dict[name_of_models[chosen_clients[i]]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        mean_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","\n","    return mean_weight_array\n","\n","\n","def strict_condition_without_outliers_set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,\n","                                                                                                       model_dict,\n","                                                                                                       iteration_distance,\n","                                                                                                       device):\n","    mean_weight_array = get_averaged_weights_without_outliers_strict_condition(model_dict, iteration_distance, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = mean_weight_array[j]\n","    return main_model\n","\n","## they do not perform any training and send same parameters that are received at the beginning at the fl round\n","def start_train_end_node_process_with_anticatalysts(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                                    batch_size, model_dict, criterion_dict, optimizer_dict,\n","                                                    numEpoch, byzantine_node_list, device):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","######################################\n","#### a little is enough functions\n","def get_zscore_for_a_little_is_enough (number_of_samples,hostile_node_percentage):\n","#     from statistics import NormalDist is defined at the top\n","    malicious = int(number_of_samples * hostile_node_percentage)\n","    supporter = np.floor((number_of_samples / 2) + 1) - malicious\n","    area = (number_of_samples - malicious - supporter) / (number_of_samples - malicious)\n","    zscore = NormalDist().inv_cdf(area)\n","    return zscore\n","def get_byzantine_node_stats_for_a_little(model_dict,byzantine_node_list, device):\n","    name_of_models = []\n","    for node in byzantine_node_list:\n","        name_of_models.append(\"model\"+str(node))\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(byzantine_node_list))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(byzantine_node_list)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","        mean_weight_array = []\n","        std_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","            std_weight_array.append(torch.std(weight_dict[weight_names_list[m]], 0))\n","    return mean_weight_array,std_weight_array\n","def change_parameters_of_hostile_nodes(model_dict, byzantine_node_list,zscore, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array,std_weight_array = get_byzantine_node_stats_for_a_little(model_dict,byzantine_node_list, device=device)\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","            for k in range(len(hostile_node_param_data_list)):\n","                hostile_node_param_data_list[k].data= mean_weight_array[k]-std_weight_array[k]*zscore\n","            model_dict[name_of_models[j]].float()\n","    return model_dict\n","#####################################\n","def get_trimmed_mean(model_dict, hostile_node_percentage, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","    mean_weight_array = []\n","    for m in range(len(weight_names_list)):\n","        layers_from_nodes = weight_dict[weight_names_list[m]]\n","        trim_layer_info=stats.trim_mean(layers_from_nodes.clone().cpu(), hostile_node_percentage, axis=0)\n","        mean_weight_array.append(trim_layer_info)\n","    return mean_weight_array\n","def set_trimmed_mean_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, hostile_node_percentage, device):\n","    mean_weight_array = get_trimmed_mean(model_dict, hostile_node_percentage, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","                        main_model_param_data_list[j].data = torch.tensor(mean_weight_array[j], dtype=torch.float32, device=device)\n","    return main_model\n","######################################\n","# fang partial knowledge attack adaptation\n","def partial_knowledge_fang_ind(main_model, model_dict,byzantine_node_list, iteration_byzantine_seed, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array, std_weight_array = get_byzantine_node_stats_for_a_little(model_dict, byzantine_node_list,\n","                                                                                       device=device)\n","        main_model_param_data_list = list(main_model.parameters())\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","            for k in range(len(hostile_node_param_data_list)):\n","                original_shape = list(hostile_node_param_data_list[k].data.shape)\n","                data = np.zeros(original_shape)\n","                hostile_data = hostile_node_param_data_list[k].data.clone().data.cpu()\n","                main_model_data = main_model_param_data_list[k].data.clone().data.cpu()\n","                mean = mean_weight_array[k].clone().data.cpu()\n","                std = std_weight_array[k].clone().data.cpu()\n","                sign_matrix = (hostile_data > main_model_data)\n","                np.random.seed(iteration_byzantine_seed) ## nodelar kendi mean stdlerine göre alıyor her experimentin x. roundı aynı gibi\n","                data[sign_matrix == True] = np.random.uniform(\n","                    low=mean[sign_matrix == True] - 4 * std[sign_matrix == True],\n","                    high=mean[sign_matrix == True] - 3 * std[sign_matrix == True])\n","                np.random.seed(iteration_byzantine_seed)\n","                data[sign_matrix == False] = np.random.uniform(\n","                    low=mean[sign_matrix == False] + 3 * std[sign_matrix == False],\n","                    high=mean[sign_matrix == False] + 4 * std[sign_matrix == False])\n","                hostile_node_param_data_list[k].data = torch.tensor(data,dtype=torch.float32, device=device)\n","            model_dict[name_of_models[j]].float()\n","    return model_dict\n","def partial_knowledge_fang_org(main_model, model_dict, byzantine_node_list, iteration_byzantine_seed, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array, std_weight_array = get_byzantine_node_stats_for_a_little(model_dict, byzantine_node_list,\n","                                                                                       device=device)\n","        main_model_param_data_list = list(main_model.parameters())\n","        organized = []\n","        for k in range(len(main_model_param_data_list)):\n","            original_shape = list(main_model_param_data_list[k].data.shape)\n","            data = np.zeros(original_shape)\n","            main_model_data = main_model_param_data_list[k].clone().data.cpu()\n","            mean = mean_weight_array[k].clone().data.cpu()\n","            std = std_weight_array[k].clone().data.cpu()\n","            sign_matrix = (mean > main_model_data)\n","            np.random.seed(iteration_byzantine_seed)\n","            data[sign_matrix == True] = np.random.uniform(\n","                low=mean[sign_matrix == True] - 4 * std[sign_matrix == True],\n","                high=mean[sign_matrix == True] - 3 * std[sign_matrix == True])\n","            np.random.seed(iteration_byzantine_seed)\n","            data[sign_matrix == False] = np.random.uniform(\n","                low=mean[sign_matrix == False] + 3 * std[sign_matrix == False],\n","                high=mean[sign_matrix == False] + 4 * std[sign_matrix == False])\n","            organized.append(data)\n","    for b in byzantine_node_list:\n","        hostile_node_param_data_list = list(model_dict[name_of_models[b]].parameters())\n","        for m in range(len(hostile_node_param_data_list)):\n","            hostile_node_param_data_list[m].data = torch.tensor(organized[m], dtype=torch.float32, device=device)\n","        model_dict[name_of_models[b]].float()\n","    return model_dict\n","\n","# Contents of distribute_data.py\n","import numpy as np\n","import pandas as pd\n","import torch\n","import cv2\n","import os\n","import requests\n","import pickle\n","import gzip\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","\n","def load_mnist_data():\n","    DATA_PATH = Path(\"data\")\n","    PATH = DATA_PATH / \"mnist\"\n","\n","    PATH.mkdir(parents=True, exist_ok=True)\n","\n","    URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n","    FILENAME = \"mnist.pkl.gz\"\n","\n","    if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)\n","\n","    with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n","\n","    return x_train, y_train, x_valid, y_valid, x_test, y_test\n","\n","\n","def load_cifar_data():\n","    transform = transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                           download=True, transform=transform)\n","\n","    x_train = torch.zeros((50000, 3, 32, 32))\n","    y_train = torch.zeros(50000)\n","    ind_train = 0\n","    for data, output in trainset:\n","        x_train[ind_train, :, :, :] = data\n","        y_train[ind_train] = output\n","        ind_train = ind_train + 1\n","\n","    x_test = torch.zeros((10000, 3, 32, 32))\n","    y_test = torch.zeros(10000)\n","    ind_test = 0\n","    for data, output in testset:\n","        x_test[ind_test, :, :, :] = data\n","        y_test[ind_test] = output\n","        ind_test = ind_test + 1\n","\n","    y_train = y_train.type(torch.LongTensor)\n","    y_test = y_test.type(torch.LongTensor)\n","\n","    return x_train, y_train, x_test, y_test\n","\n","\n","\n","def show_grid_cifar(x_data,y_data, row,column):\n","    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","    fig, axes = plt.subplots(row,column,figsize=(8,8))\n","    for i in range(row):\n","        for j in range(column):\n","            num_index = np.random.randint(len(x_data))\n","            img=x_data[num_index,:,:,:]\n","            img = img / 2 + 0.5     # unnormalize\n","            npimg = img.numpy()\n","            npimg =np.transpose(npimg, (1, 2, 0))\n","            axes[i,j].imshow(npimg)\n","\n","            axes[i,j].axis(\"off\")\n","            axes[i,j].set_title(classes[int(y_data[num_index])])\n","    plt.show()\n","\n","def load_fashion_mnist_data():\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","    testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","    x_train = trainset.data\n","    x_train = x_train / 255\n","    y_train = trainset.targets\n","\n","    x_test = testset.data\n","    x_test = x_test / 255\n","    y_test = testset.targets\n","\n","    return x_train, y_train, x_test, y_test\n","\n","\n","def show_grid_fashion_mnist(x_data, y_data, row, column):\n","    classes = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n","    fig, axes = plt.subplots(row, column, figsize=(8, 8))\n","    for i in range(row):\n","        for j in range(column):\n","            num_index = np.random.randint(len(x_data))\n","\n","            axes[i, j].imshow(x_data[num_index], cmap=\"gray\")\n","            axes[i, j].axis(\"off\")\n","            axes[i, j].set_title(classes[int(y_data[num_index])])\n","    plt.show()\n","\n","\n","def split_and_shuffle_labels(y_data, seed, amount):\n","    y_data = pd.DataFrame(y_data, columns=[\"labels\"])\n","    y_data[\"i\"] = np.arange(len(y_data))\n","    label_dict = dict()\n","    for i in range(10):\n","        var_name = \"label\" + str(i)\n","        label_info = y_data[y_data[\"labels\"] == i]\n","        np.random.seed(seed)\n","        label_info = np.random.permutation(label_info)\n","        label_info = label_info[0:amount]\n","        label_info = pd.DataFrame(label_info, columns=[\"labels\", \"i\"])\n","        label_dict.update({var_name: label_info})\n","    return label_dict\n","\n","\n","\n","def get_info_for_distribute_non_iid_with_different_n_and_amount(number_of_samples, n, amount, seed, min_n_each_node=2):\n","    node_label_info = np.ones([number_of_samples, n]) * -1\n","    columns = []\n","    for j in range(n):\n","        columns.append(\"s\" + str(j))\n","    node_label_info = pd.DataFrame(node_label_info, columns=columns, dtype=int)\n","\n","    np.random.seed(seed)\n","    seeds = np.random.choice(number_of_samples * n * 5, size=number_of_samples, replace=False)\n","    for i in range(number_of_samples):\n","        np.random.seed(seeds[i])\n","        how_many_label_created = np.random.randint(\n","            n + 1 - min_n_each_node) + min_n_each_node  ## ensures at least one label is created by default\n","        which_labels = np.random.choice(10, size=how_many_label_created, replace=False)\n","        node_label_info.iloc[i, 0:len(which_labels)] = which_labels\n","\n","    #################################\n","    #################################\n","\n","    total_label_occurences = pd.DataFrame()\n","    for m in range(10):\n","\n","        total_label_occurences.loc[0, m] = int(np.sum(node_label_info.values == m))\n","        if total_label_occurences.loc[0, m] == 0:\n","            total_label_occurences.loc[1, m] = 0\n","        else:\n","            total_label_occurences.loc[1, m] = int(amount / np.sum(node_label_info.values == m))\n","    total_label_occurences = total_label_occurences.astype('int32')\n","\n","    ##################################\n","    ##################################\n","\n","    amount_info_table = pd.DataFrame(np.zeros([number_of_samples, n]), dtype=int)\n","    for a in range(number_of_samples):\n","        for b in range(n):\n","            if node_label_info.iloc[a, b] == -1:\n","                amount_info_table.iloc[a, b] = 0\n","            else:\n","                amount_info_table.iloc[a, b] = total_label_occurences.iloc[1, node_label_info.iloc[a, b]]\n","\n","    return node_label_info, total_label_occurences, amount_info_table\n","\n","\n","def distribute_mnist_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                          x_data, y_data, x_name, y_name, node_label_info,\n","                                          amount_info_table, is_cnn=False):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","        if is_cnn:\n","            reshape_size = int(np.sqrt(x_info.shape[1]))\n","            x_info = x_info.view(-1, 1, reshape_size, reshape_size)\n","\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def distribute_fashion_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                            x_data, y_data, x_name, y_name, node_label_info, amount_info_table):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","                #                 print(label, \", start:\", index_counter.loc[label,\"start\"], \", end:\", index_counter.loc[label,\"end\"] )\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","\n","        x_info = x_info.view(-1, 1, 28, 28)\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def distribute_cifar_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                          x_data, y_data, x_name, y_name, node_label_info,\n","                                          amount_info_table):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def create_just_data(x_data, y_data, x_just_name, y_just_name):\n","    x_just_dict = dict()\n","    y_just_dict = dict()\n","\n","    for i in range(10):\n","        xname = x_just_name + str(i)\n","        x_info = x_data[y_data == i]\n","        x_just_dict.update({xname: x_info})\n","\n","        yname = y_just_name + str(i)\n","        y_info = y_data[y_data == i]\n","        y_just_dict.update({yname: y_info})\n","\n","    return x_just_dict, y_just_dict\n","\n","\n","def get_equal_size_test_data_from_each_label(x_test, y_test, min_amount=890):\n","    y_test_eq=pd.DataFrame(y_test, columns=[\"labels\"])\n","    y_test_eq[\"ind\"]=np.arange(len(y_test))\n","    hold=pd.DataFrame(columns=[\"labels\", \"ind\"])\n","    for i in range(10):\n","        hold=pd.concat([hold,y_test_eq[y_test_eq[\"labels\"]==i].iloc[0:min_amount,:] ])\n","    indices=np.array(hold[\"ind\"], dtype=int)\n","    x_test=x_test[indices, :]\n","    y_test=y_test[indices]\n","    return x_test, y_test\n","\n","\n","def choose_nodes_randomly_to_convert_hostile(hostile_node_percentage, number_of_samples, hostility_seed=90):\n","    nodes_list=[]\n","    np.random.seed(hostility_seed)\n","    nodes=np.random.choice(number_of_samples, size=int(number_of_samples*hostile_node_percentage), replace=False)\n","    for node in nodes:\n","        name=\"y_train\"+str(node)\n","        nodes_list.append(name)\n","    return nodes_list\n","\n","def convert_nodes_to_hostile(y_dict, nodes_list,\n","                             converter_dict={0:9,1:7, 2:5,3:8, 4:6, 5:2, 6:4, 7:1, 8:3, 9:0}):\n","    for node in nodes_list:\n","        original_data=y_dict[node]\n","        converted_data=np.ones(y_dict[node].shape, dtype=int)*-1\n","        labels_in_node=np.unique(original_data)\n","        for label in labels_in_node:\n","            converted_data[original_data==label]=converter_dict[label]\n","        converted_data=(torch.tensor(converted_data)).type(torch.LongTensor)\n","        y_dict.update({node:converted_data})\n","    return y_dict\n","\n","\n","\n","def create_different_converters_for_each_attacker(y_dict, nodes_list, converters_seed):\n","    converters = dict()\n","    np.random.seed(converters_seed)\n","    converter_seeds_array = np.random.choice(5000, size=len(nodes_list), replace=False)\n","\n","    for i in range(len(nodes_list)):\n","        unique_labels = np.unique(y_dict[nodes_list[i]])\n","        np.random.seed([converter_seeds_array[i]])\n","        subseeds = np.random.choice(1000, len(unique_labels), replace=False)\n","\n","        conv = dict()\n","        for j in range(len(unique_labels)):\n","            choose_from = np.delete(np.arange(10), unique_labels[j])\n","            np.random.seed(subseeds[j])\n","            chosen = np.random.choice(choose_from, replace=False)\n","            conv[unique_labels[j]] = chosen\n","        converters.update({nodes_list[i]: conv})\n","    return converters\n","\n","def convert_nodes_to_hostile_with_different_converters(y_dict, nodes_list, converters_seed=61):\n","    converters= create_different_converters_for_each_attacker(y_dict, nodes_list, converters_seed)\n","    y_dict_converted = y_dict.copy()\n","    for node in nodes_list:\n","        original_data=y_dict[node]\n","        converted_data=np.ones(y_dict[node].shape, dtype=int)*-1\n","        labels_in_node=np.unique(original_data)\n","        for label in labels_in_node:\n","            converted_data[original_data==label]=converters[node][label]\n","        converted_data=(torch.tensor(converted_data)).type(torch.LongTensor)\n","        y_dict_converted.update({node:converted_data})\n","    return y_dict_converted\n","\n","\n","def get_byzantine_node_list(hostile_node_percentage, number_of_samples, hostility_seed=90):\n","\n","    np.random.seed(hostility_seed)\n","    nodes=np.random.choice(number_of_samples, size=int(number_of_samples*hostile_node_percentage), replace=False)\n","    return nodes\n","\n","#main script\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device)\n","\n","number_of_samples = 100 #number of participants\n","\n","is_noniid = True\n","if is_noniid:\n","    n = 5\n","    min_n_each_node = 5\n","else:\n","    n = 10\n","    min_n_each_node = 10\n","\n","is_organized = True\n","hostile_node_percentage = 0.20 #malicious participant ratio\n","byzantine_mean = 0\n","byzantine_std = 1\n","\n","iteration_num = 1 #number of communication rounds(changed 500 to 4)\n","learning_rate = 0.0015\n","min_lr = 0.000010\n","lr_scheduler_factor = 0.2\n","best_threshold = 0.0001\n","clipping = True\n","clipping_threshold = 10\n","\n","weight_decay = 0.0001\n","numEpoch = 5 #changed 10 to 5\n","batch_size = 100\n","momentum = 0.9\n","\n","seed = 7\n","use_seed = 17\n","hostility_seed = 33\n","converters_seed = 221\n","byzantine_seed = 96\n","factor = 1.5\n","\n","train_amount = 500 #changed 5000 to 500\n","test_amount = 1000\n","\n","x_train, y_train, x_test, y_test = load_cifar_data()\n","\n","##train\n","label_dict_train = split_and_shuffle_labels(y_data=y_train, seed=seed, amount=train_amount)\n","node_label_info_train, total_label_occurences_train, amount_info_table_train = get_info_for_distribute_non_iid_with_different_n_and_amount(\n","    number_of_samples=number_of_samples, n=n, amount=train_amount, seed=use_seed, min_n_each_node=min_n_each_node)\n","\n","x_train_dict, y_train_dict = distribute_cifar_data_to_participants(label_dict=label_dict_train,\n","                                                                      amount=train_amount,\n","                                                                      number_of_samples=number_of_samples,\n","                                                                      n=n, x_data=x_train,\n","                                                                      y_data=y_train,\n","                                                                      node_label_info=node_label_info_train,\n","                                                                      amount_info_table=amount_info_table_train,\n","                                                                      x_name=\"x_train\",\n","                                                                      y_name=\"y_train\")\n","\n","## test\n","label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=seed, amount=test_amount)\n","node_label_info_test, total_label_occurences_test, amount_info_table_test = get_info_for_distribute_non_iid_with_different_n_and_amount(\n","    number_of_samples=number_of_samples,\n","    n=n, amount=test_amount, seed=use_seed, min_n_each_node=min_n_each_node)\n","x_test_dict, y_test_dict = distribute_cifar_data_to_participants(label_dict=label_dict_test,\n","                                                                    amount=test_amount,\n","                                                                    number_of_samples=number_of_samples,\n","                                                                    n=n, x_data=x_test,\n","                                                                    y_data=y_test,\n","                                                                    node_label_info=node_label_info_test,\n","                                                                    amount_info_table=amount_info_table_test,\n","                                                                    x_name=\"x_test\",\n","                                                                    y_name=\"y_test\")\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","test_ds = TensorDataset(x_test, y_test)\n","test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","main_model = Cifar10CNN()\n","weights_init(main_model)\n","main_model = main_model.to(device)\n","\n","main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=momentum,\n","                                 weight_decay=weight_decay)\n","main_criterion = nn.CrossEntropyLoss()\n","\n","\n","#changed this one to remove verbose=true error\n","scheduler = lr_scheduler.ReduceLROnPlateau(main_optimizer, mode=\"max\", factor=lr_scheduler_factor,\n","                                           patience=10, threshold=best_threshold, min_lr=min_lr)\n","#scheduler = lr_scheduler.ReduceLROnPlateau(main_optimizer, mode=\"max\", factor=lr_scheduler_factor,\n","                                           #patience=10, threshold=best_threshold, verbose=True, min_lr=min_lr)\n","\n","model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_cifar_cnn(number_of_samples,\n","                                                                                                    learning_rate,\n","                                                                                                    momentum, device,\n","                                                                                                    weight_decay)\n","\n","test_accuracies_of_each_iteration = np.array([], dtype=float)\n","# Create a list to store the iteration numbers\n","iteration_numbers = []  # Added for plotting and CSV file\n","\n","byzantine_node_list = get_byzantine_node_list(hostile_node_percentage, number_of_samples, hostility_seed)\n","np.random.seed(byzantine_seed)\n","byzantine_seeds_array = np.random.choice(5000, size=iteration_num, replace=False)\n","\n","for iteration in range(iteration_num):\n","    iteration_numbers.append(iteration + 1)  # Added for plotting and CSV file\n","\n","    model_dict = send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)\n","\n","    if is_organized:\n","        iteration_byzantine_seed = byzantine_seeds_array[iteration]\n","    else:\n","        iteration_byzantine_seed = None\n","\n","    start_train_end_node_process_byzantine_for_cifar_with_augmentation(number_of_samples, x_train_dict, y_train_dict,\n","                                                                          x_test_dict, y_test_dict,\n","                                                                          batch_size, model_dict, criterion_dict,\n","                                                                          optimizer_dict,\n","                                                                          numEpoch, byzantine_node_list, byzantine_mean,\n","                                                                          byzantine_std,\n","                                                                          device, clipping, clipping_threshold,\n","                                                                          iteration_byzantine_seed)\n","\n","    main_model = set_coordinatewise_med_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device)\n","    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion, device)\n","    scheduler.step(test_accuracy)\n","    new_lr = main_optimizer.param_groups[0][\"lr\"]\n","    print(f\"Current learning rate: {new_lr}\") #added this line\n","    optimizer_dict = update_learning_rate_decay(optimizer_dict, new_lr)\n","\n","    test_accuracies_of_each_iteration = np.append(test_accuracies_of_each_iteration, test_accuracy)\n","    print(\"Iteration\", str(iteration + 1), \": main_model accuracy on all test data: {:7.4f}\".format(test_accuracy))\n","\n","# Save the results to a CSV file\n","results_df = pd.DataFrame({'Iteration': iteration_numbers, 'Test Accuracy': test_accuracies_of_each_iteration})  # Added for CSV file\n","results_df.to_csv('results.csv', index=False)  # Added for CSV file\n","\n","# Plot the graph\n","plt.figure(figsize=(10, 6))  # Added for plotting\n","plt.plot(iteration_numbers, test_accuracies_of_each_iteration)  # Added for plotting\n","plt.title('Accuracy Summary')\n","plt.xlabel('Iteration Number')\n","plt.ylabel('Test Accuracy')\n","plt"]}]}