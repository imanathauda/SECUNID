{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMrU03ZJcr6vSg5GmRRPFU7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wK4qKnLG53B5","outputId":"a6331e40-4213-43e9-9d26-a16f1071af51"},"outputs":[{"output_type":"stream","name":"stdout","text":["device:  cpu\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 170498071/170498071 [00:04<00:00, 42519698.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import matplotlib.pyplot as plt  # Added for plotting\n","import pandas as pd  # Added for CSV file handling\n","\n","# Contents of construct_models.py\n","\n","import torch.nn.functional as F\n","\n","def weights_init(model, torch_manual_seed=2304):\n","    torch.manual_seed(torch_manual_seed)\n","    torch.cuda.manual_seed_all(torch_manual_seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","    for m in model.modules():\n","        if isinstance(m, nn.Conv2d):\n","            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","        elif isinstance(m, nn.Linear):\n","            nn.init.xavier_normal_(m.weight)\n","            nn.init.constant_(m.bias, 0)\n","        elif isinstance(m, nn.BatchNorm2d):\n","            nn.init.constant_(m.weight, 1)\n","            nn.init.constant_(m.bias, 0)\n","\n","def learning_rate_decay(optimizer_dict, decay_rate):\n","    for i in range(len(optimizer_dict)):\n","        optimizer_name = \"optimizer\" + str(i)\n","        old_lr = optimizer_dict[optimizer_name].param_groups[0][\"lr\"]\n","        optimizer_dict[optimizer_name].param_groups[0][\"lr\"] = old_lr * decay_rate\n","    return optimizer_dict\n","\n","def update_learning_rate_decay(optimizer_dict, new_lr):\n","    for i in range(len(optimizer_dict)):\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_dict[optimizer_name].param_groups[0][\"lr\"] = new_lr\n","    return optimizer_dict\n","\n","# Model architectures\n","class Net2nn(nn.Module):\n","    def __init__(self):\n","        super(Net2nn, self).__init__()\n","        self.fc1 = nn.Linear(784, 200)\n","        self.fc2 = nn.Linear(200, 200)\n","        self.fc3 = nn.Linear(200, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","class Netcnn(nn.Module):\n","    def __init__(self):\n","        super(Netcnn, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, 5)\n","        self.conv2 = nn.Conv2d(32, 64, 5)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 512)\n","        self.fc2 = nn.Linear(512, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","\n","class Netcnn_cifar(nn.Module):\n","    def __init__(self):\n","        super(Netcnn_cifar, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 32, 3)\n","        self.conv2 = nn.Conv2d(32, 64, 3)\n","        self.conv3 = nn.Conv2d(64, 64, 3)\n","        self.fc1 = nn.Linear(64 * 4 * 4, 64)\n","        self.fc2 = nn.Linear(64, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv3(x))\n","        x = x.view(-1, 64 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","class Cifar10CNN(nn.Module):\n","\n","    def __init__(self):\n","        super(Cifar10CNN, self).__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n","        self.conv2 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n","\n","\n","        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n","        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n","\n","\n","        self.conv5 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n","        self.conv6 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n","\n","\n","        self.fc1 = nn.Linear(128 * 4 * 4, 128)\n","        # self.dropout1 = nn.Dropout()\n","        self.fc2 = nn.Linear(128, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","\n","\n","        x = F.relu(self.conv3(x))\n","        x = F.relu(self.conv4(x))\n","        x = F.max_pool2d(x, 2)\n","\n","        x = F.relu(self.conv5(x))\n","        x = F.relu(self.conv6(x))\n","        x = F.max_pool2d(x, 2)\n","\n","        x = x.view(-1, 128 * 4 * 4)\n","        x = F.relu(self.fc1(x))\n","        # x = self.dropout1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","class Net_fashion(nn.Module):\n","    def __init__(self):\n","        super(Net_fashion, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 32, kernel_size=5, padding=2)\n","        self.conv2 = nn.Conv2d(32, 64, kernel_size=5, padding=2)\n","        self.fc1 = nn.Linear(64 * 7 * 7, 500)\n","        self.fc2 = nn.Linear(500, 10)\n","\n","\n","    def forward(self, x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x, 2)\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x, 2)\n","        x = x.view(-1, 64 * 7 * 7)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","\n","        return x\n","\n","\n","# Contents of train_nodes.py\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch import nn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","from torchvision import models\n","from torchvision import transforms\n","from statistics import NormalDist\n","from scipy import stats\n","\n","\n","def train(model, train_loader, criterion, optimizer, device):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","def train_with_clipping(model, train_loader, criterion, optimizer, device, clipping=True, clipping_threshold=10):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        if clipping:\n","            torch.nn.utils.clip_grad_value_(model.parameters(), clipping_threshold)\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","\n","def train_with_augmentation(model, train_loader, criterion, optimizer, device, clipping, clipping_threshold=10, use_augmentation=False, augment=None ):\n","    model.train()\n","    train_loss = 0.0\n","    correct = 0\n","\n","    for data, target in train_loader:\n","        data, target = data.to(device), target.to(device)\n","\n","        if use_augmentation:\n","            data = augment(data)\n","\n","        output = model(data)\n","        loss = criterion(output, target)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        if clipping:\n","            torch.nn.utils.clip_grad_value_(model.parameters(), clipping_threshold)\n","\n","        optimizer.step()\n","\n","        train_loss += loss.item()\n","        prediction = output.argmax(dim=1, keepdim=True)\n","        correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    return train_loss / len(train_loader), correct / len(train_loader.dataset)\n","\n","\n","def validation(model, test_loader, criterion, device):\n","    model.eval()\n","    test_loss = 0.0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","\n","            test_loss += criterion(output, target).item()\n","            prediction = output.argmax(dim=1, keepdim=True)\n","            correct += prediction.eq(target.view_as(prediction)).sum().item()\n","\n","    test_loss /= len(test_loader)\n","    correct /= len(test_loader.dataset)\n","\n","    return (test_loss, correct)\n","\n","\n","def get_model_names(model_dict):\n","    name_of_models = list(model_dict.keys())\n","    return name_of_models\n","\n","\n","def get_optimizer_names(optimizer_dict):\n","    name_of_optimizers = list(optimizer_dict.keys())\n","    return name_of_optimizers\n","\n","\n","def get_criterion_names(criterion_dict):\n","    name_of_criterions = list(criterion_dict.keys())\n","    return name_of_criterions\n","\n","\n","def get_x_train_sets_names(x_train_dict):\n","    name_of_x_train_sets = list(x_train_dict.keys())\n","    return name_of_x_train_sets\n","\n","\n","def get_y_train_sets_names(y_train_dict):\n","    name_of_y_train_sets = list(y_train_dict.keys())\n","    return name_of_y_train_sets\n","\n","\n","def get_x_valid_sets_names(x_valid_dict):\n","    name_of_x_valid_sets = list(x_valid_dict.keys())\n","    return name_of_x_valid_sets\n","\n","\n","def get_y_valid_sets_names(y_valid_dict):\n","    name_of_y_valid_sets = list(y_valid_dict.keys())\n","    return name_of_y_valid_sets\n","\n","\n","def get_x_test_sets_names(x_test_dict):\n","    name_of_x_test_sets = list(x_test_dict.keys())\n","    return name_of_x_test_sets\n","\n","\n","def get_y_test_sets_names(y_test_dict):\n","    name_of_y_test_sets = list(y_test_dict.keys())\n","    return name_of_y_test_sets\n","\n","\n","def create_model_optimizer_criterion_dict_for_mnist(number_of_samples, learning_rate, momentum, device, is_cnn=False, weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","        if is_cnn:\n","            model_info = Netcnn()\n","        else:\n","            model_info = Net2nn()\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def create_model_optimizer_criterion_dict_for_cifar_net(number_of_samples, learning_rate, momentum, device,\n","                                                        weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Netcnn_cifar()\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","def create_model_optimizer_criterion_dict_for_cifar_cnn(number_of_samples, learning_rate, momentum, device,\n","                                                        weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Cifar10CNN()\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum,\n","                                         weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","def create_model_optimizer_criterion_dict_for_fashion_mnist(number_of_samples, learning_rate, momentum, device, weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = Net_fashion()\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def create_model_optimizer_criterion_dict_for_cifar_resnet(number_of_samples, learning_rate, momentum, device,\n","                                                           weight_decay=0):\n","    model_dict = dict()\n","    optimizer_dict = dict()\n","    criterion_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        model_name = \"model\" + str(i)\n","\n","        model_info = models.resnet18(num_classes=10)\n","\n","        model_info = model_info.to(device)\n","        model_dict.update({model_name: model_info})\n","\n","        optimizer_name = \"optimizer\" + str(i)\n","        optimizer_info = torch.optim.SGD(model_info.parameters(), lr=learning_rate, momentum=momentum,\n","                                         weight_decay=weight_decay)\n","        optimizer_dict.update({optimizer_name: optimizer_info})\n","\n","        criterion_name = \"criterion\" + str(i)\n","        criterion_info = nn.CrossEntropyLoss()\n","        criterion_dict.update({criterion_name: criterion_info})\n","\n","    return model_dict, optimizer_dict, criterion_dict\n","\n","\n","def send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples):\n","    name_of_models = list(model_dict.keys())\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for i in range(number_of_samples):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(main_model_param_data_list)):\n","                sample_param_data_list[j].data = main_model_param_data_list[j].data.clone()\n","    return model_dict\n","\n","\n","def compare_local_and_merged_model_performance(number_of_samples, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, main_model,\n","                                               main_criterion, device):\n","    accuracy_table = pd.DataFrame(data=np.zeros((number_of_samples, 3)),\n","                                  columns=[\"sample\", \"local_ind_model\", \"merged_main_model\"])\n","\n","    name_of_x_test_sets = list(x_test_dict.keys())\n","    name_of_y_test_sets = list(y_test_dict.keys())\n","\n","    name_of_models = list(model_dict.keys())\n","    name_of_criterions = list(criterion_dict.keys())\n","\n","    for i in range(number_of_samples):\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","\n","        individual_loss, individual_accuracy = validation(model, test_dl, criterion, device)\n","        main_loss, main_accuracy = validation(main_model, test_dl, main_criterion, device)\n","\n","        accuracy_table.loc[i, \"sample\"] = \"sample \" + str(i)\n","        accuracy_table.loc[i, \"local_ind_model\"] = individual_accuracy\n","        accuracy_table.loc[i, \"merged_main_model\"] = main_accuracy\n","\n","    return accuracy_table\n","\n","\n","def start_train_end_node_process_without_print(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                               device):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","def start_train_end_node_process_with_cliiping(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                               device, clipping=True, clipping_threshold=10):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_clipping(model, train_dl, criterion, optimizer, device, clipping, clipping_threshold)\n","\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","\n","def start_train_end_node_process_cifar(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                       batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch,\n","                                       device,clipping=False, clipping_threshold =10):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    transform_augment = transforms.Compose(\n","        [\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop((32, 32), padding=4)])\n","\n","    for i in range(number_of_samples):\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_augmentation(model, train_dl, criterion, optimizer, device,\n","                                                                 clipping=clipping,\n","                                                                 clipping_threshold=clipping_threshold,\n","                                                                 use_augmentation=True, augment=transform_augment)\n","\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","##########################################\n","\n","def start_train_end_node_process_byzantine_for_cifar_with_augmentation(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict, numEpoch, byzantine_node_list,\n","                                            byzantine_mean, byzantine_std, device, clipping=False, clipping_threshold =10, iteration_byzantine_seed=None ):\n","\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","    transform_augment = transforms.Compose(\n","        [\n","            transforms.RandomHorizontalFlip(),\n","            transforms.RandomCrop((32, 32), padding=4)])\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train_with_augmentation(model, train_dl, criterion, optimizer, device, clipping=clipping, clipping_threshold=clipping_threshold,\n","                                                                 use_augmentation=True, augment=transform_augment)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","    with torch.no_grad():\n","\n","        for j in byzantine_node_list:\n","\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","\n","            for k in range(len(hostile_node_param_data_list)):\n","                np.random.seed(iteration_byzantine_seed)\n","                hostile_node_param_data_list[k].data = torch.tensor(np.random.normal(byzantine_mean,byzantine_std, hostile_node_param_data_list[k].data.shape ), dtype=torch.float32, device=device)\n","\n","            model_dict[name_of_models[j]].float()\n","\n","\n","###############################################\n","\n","def start_train_end_node_process_byzantine(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                               batch_size, model_dict, criterion_dict, optimizer_dict,\n","                                               numEpoch, byzantine_node_list, byzantine_mean, byzantine_std, device, iteration_byzantine_seed=None ):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","    with torch.no_grad():\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","\n","            for k in range(len(hostile_node_param_data_list)):\n","                np.random.seed(iteration_byzantine_seed)\n","                hostile_node_param_data_list[k].data = torch.tensor(np.random.normal(byzantine_mean,byzantine_std, hostile_node_param_data_list[k].data.shape ), dtype=torch.float32, device=device)\n","\n","            model_dict[name_of_models[j]].float()\n","\n","\n","################################################\n","\n","def calculate_euclidean_distances(main_model, model_dict):\n","    calculated_parameter_names = []\n","\n","    for parameters in main_model.named_parameters():  ## bias dataları için distance hesaplamıyorum\n","        if \"bias\" not in parameters[0]:\n","            calculated_parameter_names.append(parameters[0])\n","\n","    columns = [\"model\"] + calculated_parameter_names\n","    distances = pd.DataFrame(columns=columns)\n","    model_names = list(model_dict.keys())\n","\n","    main_model_weight_dict = {}\n","    for parameter in main_model.named_parameters():\n","        name = parameter[0]\n","        weight_info = parameter[1]\n","        main_model_weight_dict.update({name: weight_info})\n","\n","    with torch.no_grad():\n","        for i in range(len(model_names)):\n","            distances.loc[i, \"model\"] = model_names[i]\n","            sample_node_parameter_list = list(model_dict[model_names[i]].named_parameters())\n","            for j in sample_node_parameter_list:\n","                if j[0] in calculated_parameter_names:\n","                    distances.loc[i, j[0]] = round(\n","                        np.linalg.norm(main_model_weight_dict[j[0]].cpu().data - j[1].cpu().data), 4)\n","\n","    return distances\n","\n","\n","def calculate_lower_and_upper_limit(data, factor):\n","    quantiles = data.quantile(q=[0.25, 0.50, 0.75]).values\n","    q1 = quantiles[0]\n","    q2 = quantiles[1]\n","    q3 = quantiles[2]\n","    iqr = q3 - q1\n","    lower_limit = q1 - factor * iqr\n","    upper_limit = q3 + factor * iqr\n","    return lower_limit, upper_limit\n","\n","\n","def get_outlier_situation_and_thresholds_for_layers(distances, factor=1.5):\n","    layers = list(distances.columns)\n","    layers.remove(\"model\")\n","    threshold_columns = []\n","    for layer in layers:\n","        threshold_columns.append((layer + \"_lower\"))\n","        threshold_columns.append((layer + \"_upper\"))\n","    thresholds = pd.DataFrame(columns=threshold_columns)\n","\n","    include_calculation_result = True\n","    for layer in layers:\n","        data = distances[layer]\n","        lower, upper = calculate_lower_and_upper_limit(data, factor)\n","        lower_name = layer + \"_lower\"\n","        upper_name = layer + \"_upper\"\n","        thresholds.loc[0, lower_name] = lower\n","        thresholds.loc[0, upper_name] = upper\n","        name = layer + \"_is_in_ci\"\n","\n","        distances[name] = (distances[layer] > lower) & (distances[layer] < upper)\n","        include_calculation_result = include_calculation_result & distances[name]\n","\n","    distances[\"include_calculation\"] = include_calculation_result\n","    return distances, thresholds\n","\n","\n","def compare_individual_models_on_only_one_label(model_dict, criterion_dict, x_just_dict, y_just_dict, batch_size,\n","                                                device):\n","    columns = [\"model_name\"]\n","    label_names = []\n","    for l in range(10):\n","        label_names.append(\"label\" + str(l))\n","        columns.append(\"label\" + str(l))\n","\n","    accuracy_rec = pd.DataFrame(data=np.zeros([10, 11]), columns=columns)\n","\n","    # x_just_dict, y_just_dict = create_just_data(x_test, y_test, x_just_name=\"x_test_just_\", y_just_name=\"y_test_just_\")\n","\n","    name_of_x_test_just_sets = list(x_just_dict.keys())\n","    name_of_y_test_just_sets = list(y_just_dict.keys())\n","    name_of_models = list(model_dict.keys())\n","    name_of_criterions = list(criterion_dict.keys())\n","\n","    for i in range(len(name_of_models)):\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","\n","        accuracy_rec.loc[i, \"model_name\"] = name_of_models[i]\n","\n","        for j in range(10):\n","            x_test_just = x_just_dict[name_of_x_test_just_sets[j]]\n","            y_test_just = y_just_dict[name_of_y_test_just_sets[j]]\n","\n","            test_ds_just = TensorDataset(x_test_just, y_test_just)\n","            test_dl_just = DataLoader(test_ds_just, batch_size=batch_size * 2)\n","\n","            test_loss, test_accuracy = validation(model, test_dl_just, criterion, device)\n","\n","            accuracy_rec.loc[i, label_names[j]] = test_accuracy\n","    #             print( name_of_models[i], \">>\" ,j, \" tahmin etmesi: {:7.4f}\".format(test_accuracy))\n","    #         print(\"******************\")\n","    return accuracy_rec\n","\n","\n","def get_averaged_weights_faster(model_dict, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        mean_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","\n","    return mean_weight_array\n","\n","\n","def set_averaged_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device):\n","    mean_weight_array = get_averaged_weights_faster(model_dict, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = mean_weight_array[j]\n","    return main_model\n","\n","\n","def get_coordinate_wise_median_of_weights(model_dict, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        median_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            median_weight_array.append(torch.median(weight_dict[weight_names_list[m]], 0).values)\n","\n","    return median_weight_array\n","\n","def set_coordinatewise_med_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device):\n","    median_weight_array = get_coordinate_wise_median_of_weights(model_dict, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = median_weight_array[j]\n","    return main_model\n","\n","\n","\n","\n","\n","\n","\n","\n","def get_averaged_weights_without_outliers_strict_condition(model_dict, iteration_distance, device):\n","    chosen_clients = iteration_distance[iteration_distance[\"include_calculation\"] == True].index\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","\n","    ### mesela conv 1 için zeros [chosen client kadar, 32, 1, 5, 5] atanıyor bunları doldurup mean alacağız\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(chosen_clients))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(chosen_clients)):\n","            sample_param_data_list = list(model_dict[name_of_models[chosen_clients[i]]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","\n","        mean_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","\n","    return mean_weight_array\n","\n","\n","def strict_condition_without_outliers_set_averaged_weights_as_main_model_weights_and_update_main_model(main_model,\n","                                                                                                       model_dict,\n","                                                                                                       iteration_distance,\n","                                                                                                       device):\n","    mean_weight_array = get_averaged_weights_without_outliers_strict_condition(model_dict, iteration_distance, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","            main_model_param_data_list[j].data = mean_weight_array[j]\n","    return main_model\n","\n","## they do not perform any training and send same parameters that are received at the beginning at the fl round\n","def start_train_end_node_process_with_anticatalysts(number_of_samples, x_train_dict, y_train_dict, x_test_dict, y_test_dict,\n","                                                    batch_size, model_dict, criterion_dict, optimizer_dict,\n","                                                    numEpoch, byzantine_node_list, device):\n","    name_of_x_train_sets = get_x_train_sets_names(x_train_dict)\n","    name_of_y_train_sets = get_y_train_sets_names(y_train_dict)\n","    name_of_x_test_sets = get_x_test_sets_names(x_test_dict)\n","    name_of_y_test_sets = get_y_test_sets_names(y_test_dict)\n","    name_of_models = get_model_names(model_dict)\n","    name_of_criterions = get_criterion_names(criterion_dict)\n","    name_of_optimizers = get_optimizer_names(optimizer_dict)\n","\n","\n","\n","    trusted_nodes=  np.array(list(set(np.arange(number_of_samples)) - set(byzantine_node_list)), dtype=int)\n","\n","    ## STANDARD LOCAL MODEL TRAİNİNG PROCESS FOR TRUSTED NODES\n","    for i in trusted_nodes:\n","\n","        train_ds = TensorDataset(x_train_dict[name_of_x_train_sets[i]], y_train_dict[name_of_y_train_sets[i]])\n","        train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","        test_ds = TensorDataset(x_test_dict[name_of_x_test_sets[i]], y_test_dict[name_of_y_test_sets[i]])\n","        test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","        model = model_dict[name_of_models[i]]\n","        criterion = criterion_dict[name_of_criterions[i]]\n","        optimizer = optimizer_dict[name_of_optimizers[i]]\n","\n","        for epoch in range(numEpoch):\n","            train_loss, train_accuracy = train(model, train_dl, criterion, optimizer, device)\n","            test_loss, test_accuracy = validation(model, test_dl, criterion, device)\n","\n","\n","######################################\n","#### a little is enough functions\n","def get_zscore_for_a_little_is_enough (number_of_samples,hostile_node_percentage):\n","#     from statistics import NormalDist is defined at the top\n","    malicious = int(number_of_samples * hostile_node_percentage)\n","    supporter = np.floor((number_of_samples / 2) + 1) - malicious\n","    area = (number_of_samples - malicious - supporter) / (number_of_samples - malicious)\n","    zscore = NormalDist().inv_cdf(area)\n","    return zscore\n","def get_byzantine_node_stats_for_a_little(model_dict,byzantine_node_list, device):\n","    name_of_models = []\n","    for node in byzantine_node_list:\n","        name_of_models.append(\"model\"+str(node))\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    ##named_parameters layer adını ve datayı tuple olarak dönderiyor\n","    ##parameters sadece datayı dönderiyor\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(byzantine_node_list))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(byzantine_node_list)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","        mean_weight_array = []\n","        std_weight_array = []\n","        for m in range(len(weight_names_list)):\n","            mean_weight_array.append(torch.mean(weight_dict[weight_names_list[m]], 0))\n","            std_weight_array.append(torch.std(weight_dict[weight_names_list[m]], 0))\n","    return mean_weight_array,std_weight_array\n","def change_parameters_of_hostile_nodes(model_dict, byzantine_node_list,zscore, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array,std_weight_array = get_byzantine_node_stats_for_a_little(model_dict,byzantine_node_list, device=device)\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","            for k in range(len(hostile_node_param_data_list)):\n","                hostile_node_param_data_list[k].data= mean_weight_array[k]-std_weight_array[k]*zscore\n","            model_dict[name_of_models[j]].float()\n","    return model_dict\n","#####################################\n","def get_trimmed_mean(model_dict, hostile_node_percentage, device):\n","    name_of_models = list(model_dict.keys())\n","    parameters = list(model_dict[name_of_models[0]].named_parameters())\n","    weight_dict = dict()\n","    for k in range(len(parameters)):\n","        name = parameters[k][0]\n","        w_shape = list(parameters[k][1].shape)\n","        w_shape.insert(0, len(model_dict))\n","        weight_info = torch.zeros(w_shape, device=device)\n","        weight_dict.update({name: weight_info})\n","    weight_names_list = list(weight_dict.keys())\n","    with torch.no_grad():\n","        for i in range(len(model_dict)):\n","            sample_param_data_list = list(model_dict[name_of_models[i]].parameters())\n","            for j in range(len(weight_names_list)):\n","                weight_dict[weight_names_list[j]][i,] = sample_param_data_list[j].data.clone()\n","    mean_weight_array = []\n","    for m in range(len(weight_names_list)):\n","        layers_from_nodes = weight_dict[weight_names_list[m]]\n","        trim_layer_info=stats.trim_mean(layers_from_nodes.clone().cpu(), hostile_node_percentage, axis=0)\n","        mean_weight_array.append(trim_layer_info)\n","    return mean_weight_array\n","def set_trimmed_mean_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, hostile_node_percentage, device):\n","    mean_weight_array = get_trimmed_mean(model_dict, hostile_node_percentage, device)\n","    main_model_param_data_list = list(main_model.parameters())\n","    with torch.no_grad():\n","        for j in range(len(main_model_param_data_list)):\n","                        main_model_param_data_list[j].data = torch.tensor(mean_weight_array[j], dtype=torch.float32, device=device)\n","    return main_model\n","######################################\n","# fang partial knowledge attack adaptation\n","def partial_knowledge_fang_ind(main_model, model_dict,byzantine_node_list, iteration_byzantine_seed, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array, std_weight_array = get_byzantine_node_stats_for_a_little(model_dict, byzantine_node_list,\n","                                                                                       device=device)\n","        main_model_param_data_list = list(main_model.parameters())\n","        for j in byzantine_node_list:\n","            hostile_node_param_data_list = list(model_dict[name_of_models[j]].parameters())\n","            for k in range(len(hostile_node_param_data_list)):\n","                original_shape = list(hostile_node_param_data_list[k].data.shape)\n","                data = np.zeros(original_shape)\n","                hostile_data = hostile_node_param_data_list[k].data.clone().data.cpu()\n","                main_model_data = main_model_param_data_list[k].data.clone().data.cpu()\n","                mean = mean_weight_array[k].clone().data.cpu()\n","                std = std_weight_array[k].clone().data.cpu()\n","                sign_matrix = (hostile_data > main_model_data)\n","                np.random.seed(iteration_byzantine_seed) ## nodelar kendi mean stdlerine göre alıyor her experimentin x. roundı aynı gibi\n","                data[sign_matrix == True] = np.random.uniform(\n","                    low=mean[sign_matrix == True] - 4 * std[sign_matrix == True],\n","                    high=mean[sign_matrix == True] - 3 * std[sign_matrix == True])\n","                np.random.seed(iteration_byzantine_seed)\n","                data[sign_matrix == False] = np.random.uniform(\n","                    low=mean[sign_matrix == False] + 3 * std[sign_matrix == False],\n","                    high=mean[sign_matrix == False] + 4 * std[sign_matrix == False])\n","                hostile_node_param_data_list[k].data = torch.tensor(data,dtype=torch.float32, device=device)\n","            model_dict[name_of_models[j]].float()\n","    return model_dict\n","def partial_knowledge_fang_org(main_model, model_dict, byzantine_node_list, iteration_byzantine_seed, device):\n","    name_of_models = list(model_dict.keys())\n","    with torch.no_grad():\n","        mean_weight_array, std_weight_array = get_byzantine_node_stats_for_a_little(model_dict, byzantine_node_list,\n","                                                                                       device=device)\n","        main_model_param_data_list = list(main_model.parameters())\n","        organized = []\n","        for k in range(len(main_model_param_data_list)):\n","            original_shape = list(main_model_param_data_list[k].data.shape)\n","            data = np.zeros(original_shape)\n","            main_model_data = main_model_param_data_list[k].clone().data.cpu()\n","            mean = mean_weight_array[k].clone().data.cpu()\n","            std = std_weight_array[k].clone().data.cpu()\n","            sign_matrix = (mean > main_model_data)\n","            np.random.seed(iteration_byzantine_seed)\n","            data[sign_matrix == True] = np.random.uniform(\n","                low=mean[sign_matrix == True] - 4 * std[sign_matrix == True],\n","                high=mean[sign_matrix == True] - 3 * std[sign_matrix == True])\n","            np.random.seed(iteration_byzantine_seed)\n","            data[sign_matrix == False] = np.random.uniform(\n","                low=mean[sign_matrix == False] + 3 * std[sign_matrix == False],\n","                high=mean[sign_matrix == False] + 4 * std[sign_matrix == False])\n","            organized.append(data)\n","    for b in byzantine_node_list:\n","        hostile_node_param_data_list = list(model_dict[name_of_models[b]].parameters())\n","        for m in range(len(hostile_node_param_data_list)):\n","            hostile_node_param_data_list[m].data = torch.tensor(organized[m], dtype=torch.float32, device=device)\n","        model_dict[name_of_models[b]].float()\n","    return model_dict\n","\n","# Contents of distribute_data.py\n","import numpy as np\n","import pandas as pd\n","import torch\n","import cv2\n","import os\n","import requests\n","import pickle\n","import gzip\n","import math\n","import torchvision\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","\n","\n","def load_mnist_data():\n","    DATA_PATH = Path(\"data\")\n","    PATH = DATA_PATH / \"mnist\"\n","\n","    PATH.mkdir(parents=True, exist_ok=True)\n","\n","    URL = \"https://github.com/pytorch/tutorials/raw/master/_static/\"\n","    FILENAME = \"mnist.pkl.gz\"\n","\n","    if not (PATH / FILENAME).exists():\n","        content = requests.get(URL + FILENAME).content\n","        (PATH / FILENAME).open(\"wb\").write(content)\n","\n","    with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n","        ((x_train, y_train), (x_valid, y_valid), (x_test, y_test)) = pickle.load(f, encoding=\"latin-1\")\n","\n","    return x_train, y_train, x_valid, y_valid, x_test, y_test\n","\n","\n","def load_cifar_data():\n","    transform = transforms.Compose(\n","        [\n","            transforms.ToTensor(),\n","         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                            download=True, transform=transform)\n","\n","    testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                           download=True, transform=transform)\n","\n","    x_train = torch.zeros((50000, 3, 32, 32))\n","    y_train = torch.zeros(50000)\n","    ind_train = 0\n","    for data, output in trainset:\n","        x_train[ind_train, :, :, :] = data\n","        y_train[ind_train] = output\n","        ind_train = ind_train + 1\n","\n","    x_test = torch.zeros((10000, 3, 32, 32))\n","    y_test = torch.zeros(10000)\n","    ind_test = 0\n","    for data, output in testset:\n","        x_test[ind_test, :, :, :] = data\n","        y_test[ind_test] = output\n","        ind_test = ind_test + 1\n","\n","    y_train = y_train.type(torch.LongTensor)\n","    y_test = y_test.type(torch.LongTensor)\n","\n","    return x_train, y_train, x_test, y_test\n","\n","\n","\n","def show_grid_cifar(x_data,y_data, row,column):\n","    classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n","    fig, axes = plt.subplots(row,column,figsize=(8,8))\n","    for i in range(row):\n","        for j in range(column):\n","            num_index = np.random.randint(len(x_data))\n","            img=x_data[num_index,:,:,:]\n","            img = img / 2 + 0.5     # unnormalize\n","            npimg = img.numpy()\n","            npimg =np.transpose(npimg, (1, 2, 0))\n","            axes[i,j].imshow(npimg)\n","\n","            axes[i,j].axis(\"off\")\n","            axes[i,j].set_title(classes[int(y_data[num_index])])\n","    plt.show()\n","\n","def load_fashion_mnist_data():\n","    transform = transforms.Compose([transforms.ToTensor()])\n","\n","    trainset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n","    testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n","\n","    x_train = trainset.data\n","    x_train = x_train / 255\n","    y_train = trainset.targets\n","\n","    x_test = testset.data\n","    x_test = x_test / 255\n","    y_test = testset.targets\n","\n","    return x_train, y_train, x_test, y_test\n","\n","\n","def show_grid_fashion_mnist(x_data, y_data, row, column):\n","    classes = [\"T-shirt/Top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle Boot\"]\n","    fig, axes = plt.subplots(row, column, figsize=(8, 8))\n","    for i in range(row):\n","        for j in range(column):\n","            num_index = np.random.randint(len(x_data))\n","\n","            axes[i, j].imshow(x_data[num_index], cmap=\"gray\")\n","            axes[i, j].axis(\"off\")\n","            axes[i, j].set_title(classes[int(y_data[num_index])])\n","    plt.show()\n","\n","\n","def split_and_shuffle_labels(y_data, seed, amount):\n","    y_data = pd.DataFrame(y_data, columns=[\"labels\"])\n","    y_data[\"i\"] = np.arange(len(y_data))\n","    label_dict = dict()\n","    for i in range(10):\n","        var_name = \"label\" + str(i)\n","        label_info = y_data[y_data[\"labels\"] == i]\n","        np.random.seed(seed)\n","        label_info = np.random.permutation(label_info)\n","        label_info = label_info[0:amount]\n","        label_info = pd.DataFrame(label_info, columns=[\"labels\", \"i\"])\n","        label_dict.update({var_name: label_info})\n","    return label_dict\n","\n","\n","\n","def get_info_for_distribute_non_iid_with_different_n_and_amount(number_of_samples, n, amount, seed, min_n_each_node=2):\n","    node_label_info = np.ones([number_of_samples, n]) * -1\n","    columns = []\n","    for j in range(n):\n","        columns.append(\"s\" + str(j))\n","    node_label_info = pd.DataFrame(node_label_info, columns=columns, dtype=int)\n","\n","    np.random.seed(seed)\n","    seeds = np.random.choice(number_of_samples * n * 5, size=number_of_samples, replace=False)\n","    for i in range(number_of_samples):\n","        np.random.seed(seeds[i])\n","        how_many_label_created = np.random.randint(\n","            n + 1 - min_n_each_node) + min_n_each_node  ## ensures at least one label is created by default\n","        which_labels = np.random.choice(10, size=how_many_label_created, replace=False)\n","        node_label_info.iloc[i, 0:len(which_labels)] = which_labels\n","\n","    #################################\n","    #################################\n","\n","    total_label_occurences = pd.DataFrame()\n","    for m in range(10):\n","\n","        total_label_occurences.loc[0, m] = int(np.sum(node_label_info.values == m))\n","        if total_label_occurences.loc[0, m] == 0:\n","            total_label_occurences.loc[1, m] = 0\n","        else:\n","            total_label_occurences.loc[1, m] = int(amount / np.sum(node_label_info.values == m))\n","    total_label_occurences = total_label_occurences.astype('int32')\n","\n","    ##################################\n","    ##################################\n","\n","    amount_info_table = pd.DataFrame(np.zeros([number_of_samples, n]), dtype=int)\n","    for a in range(number_of_samples):\n","        for b in range(n):\n","            if node_label_info.iloc[a, b] == -1:\n","                amount_info_table.iloc[a, b] = 0\n","            else:\n","                amount_info_table.iloc[a, b] = total_label_occurences.iloc[1, node_label_info.iloc[a, b]]\n","\n","    return node_label_info, total_label_occurences, amount_info_table\n","\n","\n","def distribute_mnist_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                          x_data, y_data, x_name, y_name, node_label_info,\n","                                          amount_info_table, is_cnn=False):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","        if is_cnn:\n","            reshape_size = int(np.sqrt(x_info.shape[1]))\n","            x_info = x_info.view(-1, 1, reshape_size, reshape_size)\n","\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def distribute_fashion_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                            x_data, y_data, x_name, y_name, node_label_info, amount_info_table):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","                #                 print(label, \", start:\", index_counter.loc[label,\"start\"], \", end:\", index_counter.loc[label,\"end\"] )\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","\n","        x_info = x_info.view(-1, 1, 28, 28)\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def distribute_cifar_data_to_participants(label_dict, amount, number_of_samples, n,\n","                                          x_data, y_data, x_name, y_name, node_label_info,\n","                                          amount_info_table):\n","    label_names = list(label_dict)\n","    label_dict_data = pd.DataFrame(columns=[\"labels\", \"i\"])\n","\n","    for a in label_names:\n","        data = pd.DataFrame.from_dict(label_dict[a])\n","        label_dict_data = pd.concat([label_dict_data, data], ignore_index=True)\n","\n","    index_counter = pd.DataFrame(label_names, columns=[\"labels\"])\n","    index_counter[\"start\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","    index_counter[\"end\"] = np.ones(10, dtype=int) * np.arange(10) * amount\n","\n","    x_data_dict = dict()\n","    y_data_dict = dict()\n","\n","    for i in range(number_of_samples):\n","        node_data_indices = pd.DataFrame()\n","\n","        xname = x_name + str(i)\n","        yname = y_name + str(i)\n","\n","        for j in range(n):\n","            label = node_label_info.iloc[i, j]\n","            if label != -1:\n","                label_amount = amount_info_table.iloc[i, j]\n","                index_counter.loc[label, \"end\"] = index_counter.loc[label, \"end\"] + label_amount\n","                node_data_indices = pd.concat([node_data_indices, label_dict_data.loc[\n","                                                                  index_counter.loc[label, \"start\"]:index_counter.loc[\n","                                                                                                        label, \"end\"] - 1,\n","                                                                  \"i\"]])\n","\n","                index_counter.loc[label, \"start\"] = index_counter.loc[label, \"end\"]\n","\n","        x_info = x_data[node_data_indices.iloc[:, 0].reset_index(drop=True), :]\n","\n","        x_data_dict.update({xname: x_info})\n","\n","        y_info = y_data[node_data_indices.iloc[:, 0].reset_index(drop=True)]\n","        y_data_dict.update({yname: y_info})\n","\n","    return x_data_dict, y_data_dict\n","\n","\n","def create_just_data(x_data, y_data, x_just_name, y_just_name):\n","    x_just_dict = dict()\n","    y_just_dict = dict()\n","\n","    for i in range(10):\n","        xname = x_just_name + str(i)\n","        x_info = x_data[y_data == i]\n","        x_just_dict.update({xname: x_info})\n","\n","        yname = y_just_name + str(i)\n","        y_info = y_data[y_data == i]\n","        y_just_dict.update({yname: y_info})\n","\n","    return x_just_dict, y_just_dict\n","\n","\n","def get_equal_size_test_data_from_each_label(x_test, y_test, min_amount=890):\n","    y_test_eq=pd.DataFrame(y_test, columns=[\"labels\"])\n","    y_test_eq[\"ind\"]=np.arange(len(y_test))\n","    hold=pd.DataFrame(columns=[\"labels\", \"ind\"])\n","    for i in range(10):\n","        hold=pd.concat([hold,y_test_eq[y_test_eq[\"labels\"]==i].iloc[0:min_amount,:] ])\n","    indices=np.array(hold[\"ind\"], dtype=int)\n","    x_test=x_test[indices, :]\n","    y_test=y_test[indices]\n","    return x_test, y_test\n","\n","\n","def choose_nodes_randomly_to_convert_hostile(hostile_node_percentage, number_of_samples, hostility_seed=90):\n","    nodes_list=[]\n","    np.random.seed(hostility_seed)\n","    nodes=np.random.choice(number_of_samples, size=int(number_of_samples*hostile_node_percentage), replace=False)\n","    for node in nodes:\n","        name=\"y_train\"+str(node)\n","        nodes_list.append(name)\n","    return nodes_list\n","\n","def convert_nodes_to_hostile(y_dict, nodes_list,\n","                             converter_dict={0:9,1:7, 2:5,3:8, 4:6, 5:2, 6:4, 7:1, 8:3, 9:0}):\n","    for node in nodes_list:\n","        original_data=y_dict[node]\n","        converted_data=np.ones(y_dict[node].shape, dtype=int)*-1\n","        labels_in_node=np.unique(original_data)\n","        for label in labels_in_node:\n","            converted_data[original_data==label]=converter_dict[label]\n","        converted_data=(torch.tensor(converted_data)).type(torch.LongTensor)\n","        y_dict.update({node:converted_data})\n","    return y_dict\n","\n","\n","\n","def create_different_converters_for_each_attacker(y_dict, nodes_list, converters_seed):\n","    converters = dict()\n","    np.random.seed(converters_seed)\n","    converter_seeds_array = np.random.choice(5000, size=len(nodes_list), replace=False)\n","\n","    for i in range(len(nodes_list)):\n","        unique_labels = np.unique(y_dict[nodes_list[i]])\n","        np.random.seed([converter_seeds_array[i]])\n","        subseeds = np.random.choice(1000, len(unique_labels), replace=False)\n","\n","        conv = dict()\n","        for j in range(len(unique_labels)):\n","            choose_from = np.delete(np.arange(10), unique_labels[j])\n","            np.random.seed(subseeds[j])\n","            chosen = np.random.choice(choose_from, replace=False)\n","            conv[unique_labels[j]] = chosen\n","        converters.update({nodes_list[i]: conv})\n","    return converters\n","\n","def convert_nodes_to_hostile_with_different_converters(y_dict, nodes_list, converters_seed=61):\n","    converters= create_different_converters_for_each_attacker(y_dict, nodes_list, converters_seed)\n","    y_dict_converted = y_dict.copy()\n","    for node in nodes_list:\n","        original_data=y_dict[node]\n","        converted_data=np.ones(y_dict[node].shape, dtype=int)*-1\n","        labels_in_node=np.unique(original_data)\n","        for label in labels_in_node:\n","            converted_data[original_data==label]=converters[node][label]\n","        converted_data=(torch.tensor(converted_data)).type(torch.LongTensor)\n","        y_dict_converted.update({node:converted_data})\n","    return y_dict_converted\n","\n","\n","def get_byzantine_node_list(hostile_node_percentage, number_of_samples, hostility_seed=90):\n","\n","    np.random.seed(hostility_seed)\n","    nodes=np.random.choice(number_of_samples, size=int(number_of_samples*hostile_node_percentage), replace=False)\n","    return nodes\n","\n","# Main script\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(\"device: \", device)\n","\n","number_of_samples = 100  # number of participants\n","\n","is_noniid = True\n","if is_noniid:\n","    n = 5\n","    min_n_each_node = 5\n","else:\n","    n = 10\n","    min_n_each_node = 10\n","\n","is_organized = True\n","hostile_node_percentage = 0.20  # malicious participant ratio\n","byzantine_mean = 0\n","byzantine_std = 1\n","\n","iteration_num = 3  # number of communication rounds / changed 500 to 3\n","learning_rate = 0.0015\n","min_lr = 0.000010\n","lr_scheduler_factor = 0.2\n","best_threshold = 0.0001\n","clipping = True\n","clipping_threshold = 10\n","\n","weight_decay = 0.0001\n","numEpoch = 10\n","batch_size = 100\n","momentum = 0.9\n","\n","seed = 7\n","use_seed = 17\n","hostility_seed = 33\n","converters_seed = 221\n","byzantine_seed = 96\n","factor = 1.5\n","\n","train_amount = 500 # changed 5000 to 500\n","test_amount = 1000\n","\n","x_train, y_train, x_test, y_test = load_cifar_data()\n","\n","# Train data preparation\n","label_dict_train = split_and_shuffle_labels(y_data=y_train, seed=seed, amount=train_amount)\n","node_label_info_train, total_label_occurences_train, amount_info_table_train = get_info_for_distribute_non_iid_with_different_n_and_amount(\n","    number_of_samples=number_of_samples, n=n, amount=train_amount, seed=use_seed, min_n_each_node=min_n_each_node)\n","\n","x_train_dict, y_train_dict = distribute_cifar_data_to_participants(label_dict=label_dict_train,\n","                                                                   amount=train_amount,\n","                                                                   number_of_samples=number_of_samples,\n","                                                                   n=n, x_data=x_train,\n","                                                                   y_data=y_train,\n","                                                                   node_label_info=node_label_info_train,\n","                                                                   amount_info_table=amount_info_table_train,\n","                                                                   x_name=\"x_train\",\n","                                                                   y_name=\"y_train\")\n","\n","# Test data preparation\n","label_dict_test = split_and_shuffle_labels(y_data=y_test, seed=seed, amount=test_amount)\n","node_label_info_test, total_label_occurences_test, amount_info_table_test = get_info_for_distribute_non_iid_with_different_n_and_amount(\n","    number_of_samples=number_of_samples,\n","    n=n, amount=test_amount, seed=use_seed, min_n_each_node=min_n_each_node)\n","x_test_dict, y_test_dict = distribute_cifar_data_to_participants(label_dict=label_dict_test,\n","                                                                    amount=test_amount,\n","                                                                    number_of_samples=number_of_samples,\n","                                                                    n=n, x_data=x_test,\n","                                                                    y_data=y_test,\n","                                                                    node_label_info=node_label_info_test,\n","                                                                    amount_info_table=amount_info_table_test,\n","                                                                    x_name=\"x_test\",\n","                                                                    y_name=\"y_test\")\n","\n","train_ds = TensorDataset(x_train, y_train)\n","train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n","\n","test_ds = TensorDataset(x_test, y_test)\n","test_dl = DataLoader(test_ds, batch_size=batch_size * 2)\n","\n","main_model = Cifar10CNN()\n","weights_init(main_model)\n","main_model = main_model.to(device)\n","\n","main_optimizer = torch.optim.SGD(main_model.parameters(), lr=learning_rate, momentum=momentum,\n","                                 weight_decay=weight_decay)\n","main_criterion = nn.CrossEntropyLoss()\n","\n","#changed this one to remove verbose=true error\n","scheduler = lr_scheduler.ReduceLROnPlateau(main_optimizer, mode=\"max\", factor=lr_scheduler_factor,\n","                                           patience=10, threshold=best_threshold, min_lr=min_lr)\n","#scheduler = lr_scheduler.ReduceLROnPlateau(main_optimizer, mode=\"max\", factor=lr_scheduler_factor,\n","                                           #patience=10, threshold=best_threshold, verbose=True, min_lr=min_lr)\n","\n","model_dict, optimizer_dict, criterion_dict = create_model_optimizer_criterion_dict_for_cifar_cnn(number_of_samples,\n","                                                                                                    learning_rate,\n","                                                                                                    momentum, device,\n","                                                                                                    weight_decay)\n","\n","test_accuracies_of_each_iteration = np.array([], dtype=float)\n","\n","# Create a list to store the iteration numbers\n","iteration_numbers = []  # Added for plotting and CSV file\n","\n","byzantine_node_list = get_byzantine_node_list(hostile_node_percentage, number_of_samples, hostility_seed)\n","np.random.seed(byzantine_seed)\n","byzantine_seeds_array = np.random.choice(5000, size=iteration_num, replace=False)\n","\n","for iteration in range(iteration_num):\n","    iteration_numbers.append(iteration + 1)  # Added for plotting and CSV file\n","    model_dict = send_main_model_to_nodes_and_update_model_dict(main_model, model_dict, number_of_samples)\n","\n","    if is_organized:\n","        iteration_byzantine_seed = byzantine_seeds_array[iteration]\n","    else:\n","        iteration_byzantine_seed = None\n","\n","    start_train_end_node_process_byzantine_for_cifar_with_augmentation(number_of_samples, x_train_dict, y_train_dict,\n","                                                                          x_test_dict, y_test_dict,\n","                                                                          batch_size, model_dict, criterion_dict,\n","                                                                          optimizer_dict,\n","                                                                          numEpoch, byzantine_node_list, byzantine_mean,\n","                                                                          byzantine_std,\n","                                                                          device, clipping, clipping_threshold,\n","                                                                          iteration_byzantine_seed)\n","\n","\n","    main_model = set_averaged_weights_as_main_model_weights_and_update_main_model(main_model, model_dict, device)\n","    test_loss, test_accuracy = validation(main_model, test_dl, main_criterion, device)\n","    scheduler.step(test_accuracy)\n","    new_lr = main_optimizer.param_groups[0][\"lr\"]\n","    print(f\"Current learning rate: {new_lr}\") #added this line\n","    optimizer_dict = update_learning_rate_decay(optimizer_dict, new_lr)\n","\n","    test_accuracies_of_each_iteration = np.append(test_accuracies_of_each_iteration, test_accuracy)\n","\n","# Save the results to a CSV file\n","results_df = pd.DataFrame({'Iteration': iteration_numbers, 'Test Accuracy': test_accuracies_of_each_iteration})  # Added for CSV file\n","results_df.to_csv('results.csv', index=False)  # Added for CSV file\n","\n","    # Plot the graph\n","plt.figure(figsize=(10, 6))  # Added for plotting\n","plt.plot(iteration_numbers, test_accuracies_of_each_iteration)  # Added for plotting\n","plt.title('Accuracy Summary')\n","plt.xlabel('Iteration Number')\n","plt.ylabel('Test Accuracy')\n","plt"]}]}